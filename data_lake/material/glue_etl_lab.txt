Lab Instructions - Glue ETL with Apache Spark - Convert Format to Parquet
Lab - Glue ETL with Apache Spark - Convert Format to Parquet
Introduction
By storing your data in an optimal format, you can lower storage and query cost while substantially speeding up query performance.

Apache Parquet is a compressed storage file format designed for efficiently querying a large amount of data.

Parquet is widely supported by popular data processing frameworks.

In this lab, let’s use Glue ETL to convert the Iris data from CSV to Parquet format.

With Glue, you can easily create and schedule transformation jobs using web console

Glue ETL automatically provisions Apache Spark environment to run the job.

Lab
From the management console, look for Glue service

Make sure you are in the same region where the S3 bucket is (I am using N.Viriginia)

From the navigation pane, Select Jobs under Data Integration and ETL -> AWS Glue Studio

You now have many options to create ETL jobs

Let’s go with the Visual with a source and target

For Source: select Amazon S3

For Target: select Amazon S3

Create Job

Edit the job name and let’s name it as - iris_csv_to_parquet

In the visual editor, select Data source – S3 bucket

Select the tab Data source properties – S3

S3 source type: choose Data Catalog table

Database: choose demo_db

Table: choose iris_csv

We need to grant permission for the ETL job to read and write to our S3 bucket.

We can do that using the IAM Role AWSGlueServiceRoleDefault

Let’s leave rest of the default configuration

Glue is going to generate Spark scripts for transformation

Apply Mapping
Choose the ApplyMapping node in the editor

We are going to leave the column names and data type as-is

Data Target
Choose the Data target – S3 bucket node in the editor

Go to the Data target properties – S3 tab

For Format: choose Parquet

Compression Type: choose Snappy

S3 Target Location: specify your S3 path s3://aws-glue-yourname/iris/parquet/

Ensure your path has a trailing slash “/” as shown above

Job Details
We now need to configure security and other details

Choose the Job details tab in the editor

IAM Role: choose AWSGlueServiceRoleDefault

Job bookmark: choose Disable

Number of retries: set it to 0

Job bookmark is used for incremental loading of data from the source. However, this feature appears to be still buggy. We are going to disable for now

Save the job

Script
The Script tab has the PySpark script generated by the Glue visual editor

You can review and modify. However, we are going to run the script as-is

Let’s run the job

Click on Run button

The job will take a few minutes to complete.

Go to the Runs tab and wait until the job completes

Glue provisions required Spark infrastructure to run the job and automatically terminates the environment after the job completes.

The job status should change to Succeeded

Output
To view the transformed Parquet files in S3, you can go the folder

s3://aws-glue-yourname/iris/parquet/

To query this file, we need to create a Glue Crawler to collect the metadata

Crawler
Create a crawler with this configuration:

a. Crawler Name: iris_parquet_crawler

b. Data source: S3

c. Path: s3://aws-glue-yourname/iris/parquet/

d. IAM Role: AWSGlueServiceRoleDefault

Configure the Crawler's output:

a. Database: demo_db

b. Prefix: iris_

c. Review and Create crawler

Run Crawler. Wait until the crawler finishes

Crawler would add a new iris_parquet table to the Glue catalog

Query
We can now query this parquet data using Athena:

SELECT sepal_length, sepal_width,
sepal_length * sepal_width as sepal_area
FROM "demo_db"."iris_parquet" limit 10;
Summary
In this lab, we looked at how to transform data with Glue ETL

Within a few minutes, we were able to generate ETL script and do the transformation

By transforming the data into an efficient format, we can lower storage and
query cost while at the same time improve query performance.
